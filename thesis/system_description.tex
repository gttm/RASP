\chapter{System Description}\label{chapter:system}

\section{System Overview}

As mentioned before, the objective of this thesis is the design and implementation of a distributed system that allows the execution of SQL queries that join a real-time data stream and an external dataset. The prerequisites for the system are scalability, fault tolerance, extensibility, but most importantly enabling the execution of low latency SQL queries. This means that the latency between the issue of the query and the moment we receive the query response must be minimized. The delay between the data generation and the moment they are available for querying must also be as small as possible, since we are dealing with a real-time data stream.

Performing an SQL join combines records from two tables. The join effectively creates a third table which combines the information from both of them. Performing a join can be expensive in terms of the time it takes to compute it, especially if one or both of the tables are large in size. Since minimizing the join query latency is our priority, we can store the stream of data combined with the external data information in a single denormalized table. \emph{Denormalization} is the process of attempting to optimize the read performance of a database by adding redundant data, an example of which can be seen in Table \ref{table:denormalization_example}. A Storm topology can compute the join of the data stream and the external dataset in real time and store the denormalized data stream at a Phoenix table in HBase.

\begin{table}[h!]
\centering
\subfloat[Employee table]{\begin{tabular}{ |c|c| }
\hline
LastName & DepartmentID \\ \hline \hline
Jones & 2 \\ \hline
Wagner & 1 \\ \hline
Gray & 1 \\ \hline
Draper & 3 \\ \hline
Nolan & 2 \\ \hline
\end{tabular}}
\quad
\subfloat[Department table]{\begin{tabular}{ |c|c| }
\hline
DepartmentID & DepartmentName \\ \hline \hline
1 & Sales \\ \hline
2 & Engineering \\ \hline
3 & Marketing \\ \hline
\end{tabular}}
\\
\subfloat[Denormalized table]{\begin{tabular}{ |c|c|c| }
\hline
LastName & DepartmentID & DepartmentName \\ \hline \hline
Jones & 2 & Engineering\\ \hline
Wagner & 1 & Sales\\ \hline
Gray & 1 & Sales\\ \hline
Draper & 3 & Marketing\\ \hline
Nolan & 2 & Engineering\\ \hline
\end{tabular}}
\caption{Denormalization example}
\label{table:denormalization_example}
\end{table}

This design decision allows all subsequent queries that combine the data stream and the external dataset to be performed directly on the denormalized Phoenix table, without the need to perform the computationally expensive join on query time. Denormalization introduces a trade-off, speeding up reads from queries while slowing down writes to the table, since the join is performed by the Storm topology. However, if the system processing rate can handle the real-time data generation rate, slower writes are not a problem.

From a high level, the system implemented for our IXP network data use case consists of 4 major parts that can be seen in Figure \ref{figure:system_overview}. In the first part, the network data is generated by the switches of an IXP and collected by a host running a Kafka producer. There, the useful fields are extracted from the headers of the captured packets and published to the Kafka topic. The second component of the system is the Kafka topic that temporarily stores the data stream at the Kafka cluster. In the next part, the data stream is processed by a Storm topology. The topology contains the IP to AS Bolt, that performs the join of the data stream and the AS dataset in-memory, since the size of the dataset is small enough. It also contains the IP to DNS Bolt, that performs the join of the data stream and the Reverse DNS dataset using \texttt{Get} operations on the HBase table where the dataset is stored, since it does not fit in the bolt's memory. Finally, in the last part the denormalized network data is stored at a Phoenix table in HBase, allowing Phoenix clients to perform low latency SQL queries to it.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/system_overview}
\caption{Storm architecture overview}
\label{figure:system_overview}
\end{figure}

The system's \textbf{scalability} is achieved by using distributed frameworks and technologies for its implementation. Kafka topics consist of partitions that are distributed over a cluster of Kafka brokers. Storm topologies run over a cluster of Supervisors and multiple instances of any component of the topology (spout or bolt) can run at the same time. The output Phoenix table is stored in HBase, and subsequently in the HDFS, which are both distributed technologies that run on clusters of DataNodes and RegionServers respectively. Moreover, Phoenix can parallelize queries to take full advantage of the HBase cluster.

\textbf{Fault tolerance} is very important for our system since it will be constantly running for extended periods of time, processing real-time data. First of all, Kafka topic partitions can be replicated across multiple Kafka brokers, allowing data input by the Kafka producer and consumption by the Storm topology even in the case of a broker failure. Storm topologies are also fault-tolerant and in case of a Supervisor failure Nimbus reassigns the tasks as necessary. Storm also keeps track of failed tuples and is able to replay them, since Kafka retains a topic's data for a configurable period of time. This allows us to restart Storm topologies without skipping any data. Finally, the output Phoenix table that is stored in HBase is replicated by the underlying HDFS, allowing its data to be available in the case of a DataNode or RegionServer failure.

Using the Storm framework provides \textbf{extensibility} to our system. Extending the functionality of the Storm topology for a new dataset is as simple as adding an extra bolt to the topology. For example, the processing for the join of the data stream with a new external dataset can be added by implementing the new bolt and placing it before the output Phoenix Bolt. We discern two cases with respect to the size of the external dataset. If the dataset's size is small enough, we can load it in the bolt's memory and perform the join in-memory. Otherwise, when the dataset does not fit in memory, we store it in an HBase table and perform the join using \texttt{Get} operations.

In the following Sections of this Chapter we offer a detailed description for all of the system's components.


\section{Data Generation and Input}

\subsection{IXP Switch}

The data stream that is processed by our system is generated by an \emph{sFlow agent} running on a switch that processes traffic in an IXP. sFlow \cite{sflow} is an industry standard technology for monitoring high speed switched networks and is supported by multiple network device manufacturers. The sFlow agent performs random sampling to the packets processed by the switch. By default, the agent samples the first 128 bytes of 1 in every 2048 packets.

The flow samples are sent as \emph{sFlow datagrams} (UDP packets) to the \emph{sFlow collector}, described in Subsection \ref{subsection:system_kafka_producer}. The sFlow collector can accept sFlow datagrams from multiple sFlow agents, allowing us to process a data stream that combines flow samples generated by multiple switches that are used in the same IXP.

\subsection{Kafka Producer}\label{subsection:system_kafka_producer}

The sFlow datagrams are sent by the sFlow agents of the IXP switches to an sFlow collector running at a specified host. This sFlow collector collects the flow samples from all of the switches and makes them available for further processing. In our implementation we use \texttt{sflowtool} \cite{sflowtool}, a tool functions as an sFlow collector and translates the flow samples to a simple-to-parse ASCII format.

The same host runs a Kafka producer script that preprocesses the flow samples and publishes the useful fields to a Kafka topic. This script reads the output of our sFlow collector \texttt{sflowtool} and extracts the following useful fields for each sampled packet:
\begin{itemize}
\item \texttt{sourceIP}: source IP address in dot-decimal notation
\item \texttt{destinationIP}: destination IP address in dot-decimal notation
\item \texttt{protocol}: IP protocol number (6 for TCP, 17 for UDP)
\item \texttt{sourcePort}: source port number
\item \texttt{destinationPort}: destination port number
\item \texttt{ipSize}: total length of the IP packet
\item \texttt{dateTime}: Unix timestamp of the packet's capture time in microseconds. This field is generated by the script while preprocessing each packet. 
\end{itemize}

After the extraction, we compose a message containing the fields in CSV format. The script is running a Kafka producer that publishes these messages to the Kafka topic \texttt{netdata} that is stored at the Kafka cluster. 

Algorithm \ref{algorithm:kafka_producer} outlines the script implementation.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\For {line in sFlowToolOutput}
\State fields = line.split(",")
\State sourceIP = fields[9]
\State destinationIP = fields[10]
\State protocol = fields[11]
\State sourcePort = fields[14]
\State destinationPort = fields[15]
\State ipSize = fields[17]
\State dateTime = int(time.time()*1000000)
\State message = "\{\},\{\},\{\},\{\},\{\},\{\},\{\}".format(sourceIP, destinationIP, protocol, sourcePort, destinationPort, ipSize, dateTime)
\State kafkaProducer.send\_messages("netdata", message)
\EndFor
\end{algorithmic}
\caption{Kafka Producer}
\label{algorithm:kafka_producer}
\end{algorithm}

Messages can be sent to a Kafka topic either synchronously or asynchronously \cite{kafka_documentation}. Synchronous send publishes the messages immediately, whereas asynchronous send accumulates them in memory batches multiple messages in a single request. As we will see in Subsection \ref{subsection:benchmarks_kafka_batch} batching can greatly increase the performance of the producer, therefore we choose to use asynchronous send.


\section{Kafka Topic}

The preprocessed messages containing the useful fields in CSV format are stored at the \texttt{netdata} Kafka topic in the Kafka cluster. To ensure scalability and load balancing, we set the number of the topic's partitions equal to the number of the brokers of the Kafka cluster. In this way, the write and read requests of the producer and the consumers respectively are distributed over the cluster.

To provide fault tolerance, we also set a replication factor of 2 for the topic. This means that every partition is replicated and stored in 2 brokers, the leader that handles all the requests for the partition, and the follower that is replicating the leader. In case of failure of the topic leader, the follower can take over and handle the requests for the partition.

As we mentioned in Section \ref{section:theory_kafka}, all published messages remain stored at the brokers for a configurable period of time, whether or not they have been consumed. This allows the Storm topology to replay previously read messages in case of failure. The default data retention window for the topic is 7 days.


\section{Storm Topology}

The Storm topology is the heart of our system. This is where the processing of the data stream is performed. The topology consists of one spout and four bolts in a pipeline setup: Kafka Spout, Split Fields Bolt, IP to AS Bolt, IP to DNS Bolt and Phoenix Bolt. In short, the topology reads messages from a Kafka topic, extracts the useful fields from the messages, performs the join of the data stream and the external datasets and finally stores the denormalized data stream in a Phoenix table. The topology has acking enabled, which guarantees that every message from the topic will be processed and will be replayed in case of failure.

The overview of the Storm topology for our network data use case can be seen in Figure \ref{figure:system_storm_topology}. The functionality of the topology can be extended by adding more bolts that perform the join of the data stream and another external dataset right before the Phoenix Bolt.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/system_storm_topology}
\caption{Storm topology overview}
\label{figure:system_storm_topology}
\end{figure}

\subsection{Kafka Spout}

The source of data stream in our topology is the Kafka Spout. The spout is a Kafka consumer that reads messages from the \texttt{netdata} Kafka topic and emits them to the Split Fields Bolt. The maximum parallelism of the Kafka spout is the number of the topic's partitions, because any instances of the spout further than that would not read any data.

The Kafka Spout stores the offset of the consumer for each partition of the topic in Zookeeper. In this way, if a failure happens the topology can be restarted and resume reading messages from the last one that was executed successfully by the topology.

\subsection{Split Fields Bolt}

The tuple emitted by the Kafka Spout has a single field: a message from the topic containing the useful fields of the packet in CSV format. The Split Fields Bolt extracts these fields from the message. In addition to that the bolt computes the integer representations of the source and destiantion IP addresses, which are usually more useful than the IP addresses in dot-decimal notation.

After processing the Kafka message, the Split Fileds Bolt emits a tuple containing the following fields: sourceIP, sourceIPInt, destinationIP, destinationIPInt, protocol, sourcePort, destinationPort, ipSize, dateTime.

\subsection{IP to AS Bolt}

The join of the data stream and the Autonomous System dataset is performed by the IP to AS Bolt. The Autonomous System dataset maps IP address ranges to AS number and name. The data contained in the dataset are stored in CSV format and have 3 fields: the first IP address contained in the AS, the last IP address contained in the AS and the AS number and name. The IP adresses are stored in their integer representation format. The dataset file must be stored in a location accessible by all of the Supervisors, such as the HDFS. Further information on the dataset is available in Subsection \ref{subsection:benchmarks_as_dataset}.

The defining characteristic of the Autonomous System dataset is that its size (13 MB) is small enough to fit in the memory, which is the optimal way to perform the join of the stream and the dataset. During the initialization of the topology the \texttt{prepare} method of the IP to AS Bolt is called and loads the dataset in a TreeMap structure. A TreeMap is a map implementation based on red-black trees, a variation of binary search trees, that allow searching in $O(\log{n})$ time \cite{treemap}. For each record of the dataset we insert two records in the TreeMap, containing the start and the stop IP address for each AS along with the AS number and name, as seen in Algorithm \ref{algorithm:ip_to_as_bolt}.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{prepare}{}
\State asMap = new TreeMap<Long, String[]>()
\For {line in ipToASFile}
\State fields = line.split(",")
\State asMap.put(fields[0], [fields[2], "start"])
\State asMap.put(fields[1], [fields[2], "stop"])
\EndFor
\EndFunction

\Function{ipToAS}{ipInt}
\State as = "null"
\State key = asMap.ceilingKey(ipInt)
\If {key != null}
\State value = asMap.get(key)
\If {(key == ipInt) || (value[1].equals("stop"))}
\State as = value[0]
\EndIf
\EndIf
\Return as
\EndFunction

\Function{execute}{tuple}
\State sourceIPInt = tuple.getField("sourceIPInt")
\State destinationIPInt = tuple.getField("destinationIPInt")
\State outputValues = tuple.getValues()
\State outputValues.add(ipToAS(sourceIPInt))
\State outputValues.add(ipToAS(destinationIPInt))
\State collector.emit(outputValues)
\EndFunction
\end{algorithmic}
\caption{IP to AS Bolt}
\label{algorithm:ip_to_as_bolt}
\end{algorithm}

The helper method \texttt{ipToAS} takes an IP address in integer representation format as input and returns the name and number of the AS it belongs. More specifically, by using the TreeMap's \texttt{ceilingKey} and \texttt{get} methods we find the first AS boundary IP address larger or equal to the input IP address. If this address is equal to the input IP address or corresponds to the last address of an AS IP address range, then the AS it belongs is the one we are looking for and its number and name is returned by the method. Otherwise the IP address provided does not belong to any AS according to the dataset and the \texttt{ipToAS} method returns the String \texttt{"null"}.

For every tuple received by the bolt, the \texttt{execute} method is called. Using the sourceIPInt and destinationIPInt fields as input to the method \texttt{ipToAS} we determine the sourceAS and destinationAS fields that denote the source and destination AS number and name respectively. The new fields are appended to the received fields and all of them are emitted to the next bolt of the topology.

\bigskip

\subsection{IP to DNS Bolt}

The join of the data stream and the Reverse DNS dataset is performed by the IP to DNS Bolt. The Reverse DNS dataset maps IP addresses to domain names. The data contained in the dataset have 2 fields: the IP address in dot-decimal notation and the corresponding domain name. Further information on the dataset is available in Subsection \ref{subsection:benchmarks_dns_dataset}.

The defining characteristic of the Reverse DNS dataset is that its size (55 GB uncompressed) is larger than the memory size, therefore loading it in every bolt's memory is not an option. To make the dataset available to the bolts, we store it in the \texttt{rnds} HBase table, where the IP addresses are used as the row key and the domain names are stored in the column \texttt{d:dns}. This allows the bolt to perform \texttt{Get} operations on the table for an IP address row key to receive the corresponding domain name.

HBase can perform low latency \texttt{Get} operations by using \emph{Bloom filters} \cite{bloom}. A Bloom filter, is a data structure which is designed to predict whether a given element is a member of a set of data. A positive result from a Bloom filter is not always accurate, but a negative result is guaranteed to be accurate. In HBase, Bloom filters a lightweight in-memory structure that reduces the number of disk reads for a given \texttt{Get} operation to only the HFiles likely to contain the desired row.

The helper method \texttt{ipToDNS} takes an IP address in dot-decimal notation as input and returns corresponding domain name. More specifically, a \texttt{Get} operation is performed on the \texttt{rdns} HBase table for the input IP address row key. If the \texttt{Get} is successful, the corresponding domain name is the value of the column \texttt{d:dns} of the returned row, and is afterwards returned by the method. Otherwise the IP address provided does not have a corresponding domain name according to the dataset and the \texttt{ipToDNS} method returns the String \texttt{"null"}.

For every tuple received by the bolt, the \texttt{execute} method is called. Using the sourceIP and destinationIP fields as input to the method \texttt{ipToDNS} we determine the sourceDNS and destinationDNS fields that denote the source and destination domain names respectively. The new fields are appended to the received fields and all of them are emitted to the next bolt of the topology.

Algorithm \ref{algorithm:ip_to_dns_bolt} outlines the IP to DNS Bolt implementation.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{ipToDNS}{ip}
\State table = new HTable("rdns")
\State g = new Get(ip)
\State res = table.get(g)
\State dns = res.getValue("d", "dns")
\If {dns == null}
\State dns = "null"
\EndIf
\Return dns
\EndFunction

\Function{execute}{tuple}
\State sourceIP = tuple.getField("sourceIP")
\State destinationIP = tuple.getField("destinationIP")
\State outputValues = tuple.getValues()
\State outputValues.add(ipToDNS(sourceIP))
\State outputValues.add(ipToDNS(destinationIP))
\State collector.emit(outputValues)
\EndFunction
\end{algorithmic}
\caption{IP to DNS Bolt}
\label{algorithm:ip_to_dns_bolt}
\end{algorithm}

\subsection{Phoenix Bolt}

The last component of the topology is the Phoenix Bolt, which inserts the denormalized data stream into the \texttt{netdata} Phoenix table. The table is described in detail in Section \ref{section:system_phoenix_table}. This bolt uses the Phoenix JDBC driver and performs an \texttt{UPSERT VALUES} query that includes all the fields received by the bolt. \texttt{UPSERT} queries are the only way to insert data in a table in Phoenix. This query inserts the row if not present, otherwise it updates the row's values in the table. In our case where the primary key of the table is the packet timestamp which is monotonically increasing this query behaves like \texttt{INSERT VALUES}.

\begin{lstlisting}[language=PhoenixSQL]
UPSERT INTO netdata VALUES (dateTime, sourceIP, sourceIPInt, destinationIP, destinationIPInt, protocol, sourcePort, destinationPort, ipSize, sourceAS, destinationAS, sourceDNS, destinationDNS);
\end{lstlisting}


\section{Phoenix Table}\label{section:system_phoenix_table}

After being computed by the Storm topology, the denormalized data stream is stored at the \texttt{netdata} Phoenix table in HBase. The design of this table is important because it affects the way queries are executed. In our use case, the queries performed will be topN AS or topN DNS queries over a time window for the data. 

The queries performed on the table have a time window constraint. To benefit from HBase \texttt{Scan} operations that perform sequential reads, we want to use the packet's capture timestamp as the row key in the underlying HBase table. In this way, the HBase table is sorted by capture timestamp and the data for any time window are stored sequentially. To achieve this in Phoenix, we use the packet's capture timestamp as the primary key of the Phoenix table. The capture timestamp in microseconds can be used as the primary key since it is unique for each packet.

The use case queries concern either AS or DNS information. In HBase only the column families needed for the query are cached. Having separate column families containing AS, DNS and other information reduces query latency by reducing the data that have to be cached during each query \cite{hbase_reference}. Therefore we separate the table's columns in 3 column families: one for the AS fields, another for DNS fields and a default column family that contains the rest of the packet's fields.

In HBase every cell value is always (when stored, transfered or cached) accompanied by its row key, column name and timestamp. Since the table will store millions of cells the column names will be repeates several millions of times in our data \cite{hbase_reference}. This means that if the column names are large then the table size will be significantly increased. This is why we try to minimize the column names by keeping the column family and column qualifier names as small as possible.

Another way to reduce the table size is by utilizing Phoenix \emph{data types}. Using the appropriate data type for each column reduces the size of each row, which improves query performance. For example, instead of storing the capture timestamp as string, we use the \texttt{BIGINT} type. The current UNIX timestamp in microseconds has 16 digits. As a string this needs 16 bytes to be stored, whereas a \texttt{BIGINT} needs only 8 bytes.

Having all the aforementioned design choices taken into consideration we create the \texttt{netdata} Phoenix table with the columns listed below. The dots in the column names separate the column families from the column qualifiers created in the underlining HBase table.
\begin{itemize}
\item \texttt{t}: Unix timestamp of the packet's capture time in microseconds, used as the primary key
\item \texttt{d.ipS}: source IP address in dot-decimal notation
\item \texttt{d.ipSI}: integer representation of the source IP address
\item \texttt{d.ipD}: destination IP address in dot-decimal notation
\item \texttt{d.ipDI}: integer representation of the destination IP address
\item \texttt{d.proto}: IP protocol number of the packet
\item \texttt{d.portS}: source port number
\item \texttt{d.portD}: destination port number
\item \texttt{d.size}: total length of the IP packet
\item \texttt{as.asS}: AS number and name of the source IP address
\item \texttt{as.asD}: AS number and name of the destination IP address
\item \texttt{dns.dnsS}: domain name of the source IP address
\item \texttt{dns.dnsD}: domain name of the destination IP address
\end{itemize}

The Phoenix SQL statement used to create the final \texttt{netdata} table, including the optimizations that will be described in Chapter \ref{chapter:optimizations}, is the following.

\begin{lstlisting}[language=PhoenixSQL]
CREATE TABLE netdata (
    t BIGINT PRIMARY KEY,
    d.ipS VARCHAR,
    d.ipSI BIGINT,
    d.ipD VARCHAR,
    d.ipDI BIGINT,
    d.proto SMALLINT,
    d.portS INTEGER,
    d.portD INTEGER,
    d.size INTEGER,
    as.asS VARCHAR,
    as.asD VARCHAR,
    dns.dnsS VARCHAR,
    dns.dnsD VARCHAR
) 
SALT_BUCKETS = 4,
DEFAULT_COLUMN_FAMILY = 'd',
DATA_BLOCK_ENCODING = 'NONE',
COMPRESSION = 'SNAPPY';
\end{lstlisting}


\cleardoublepage

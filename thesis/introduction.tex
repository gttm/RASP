\chapter{Introduction}\label{chapter:introduction}

\section{Motivation}

Over the past decades, the Internet is continuously growing, driven by ever greater amounts of online information and knowledge, commerce, entertainment and social networking. Recent studies forecast that global Internet traffic will grow with a compound annual growth rate of 26\% over the next years, reaching 136.1 Exabytes per month in 2019, up from 42.4 Exabytes per month in 2014 \cite{cisco_2014}. The key elements that will shape Internet traffic in the coming years include the increase in the number of the Internet users, the proliferation of networked devices such as tablets and smartphones, faster broadband speeds, advanced video services and increased IP traffic deriving from cellular data connections \cite{cisco_press}.

Large portions of the Internet traffic are routed through Internet Exchange Points (IXPs). An IXP consists of one or more network switches, to which Internet Service Providers (ISPs) connect and exchange Internet traffic between their networks. The IXP allows these networks to interconnect directly, rather than through their upstream transit providers, thereby reducing costs, latency and bandwidth. 

Recent studies have shown that large IXPs have visibility to a large fraction of the Internet and fit the role of being global Internet vantage points \cite{smaragdakis}. Therefore, one can extract information about the global state of the Internet by analyzing the traffic of a large IXP over a sufficient period of time. The typical approach to perform network traffic analysis on a large IXP is by sampling the traffic over a period of time and saving the capture in a file. Then the capture is processed in a centralized manner by a script, where the network traffic analysis is performed. This approach has two main drawbacks. From the one hand it does not scale for a larger amount of network data. From the other hand processing offline network traffic captures limits the ``freshness" of the data.

Over the past years, a variety of distributed technologies and frameworks have been developed to process and store big data \cite{hadoop,storm,hbase}. Distributed technologies such as Kafka, Storm, HDFS, HBase and Phoenix can be used to implement scalable systems that process and analyze data streams in real time. By using such technologies for processing and analyzing the IXP network traffic, the issues mentioned in the previous paragraph can be alleviated. 


\section{Objectives}

The objective of this thesis is the design and implementation of a distributed system that allows the execution of SQL queries that join a real-time data stream and an external dataset. The top priority of the system is minimizing the execution latency of these queries, which contains two sub-objectives. From the one hand, the latency between the issue of the query and the moment we receive the query response must be minimized. From the other hand, the delay between the data generation and the moment they are available for querying must also be as small as possible, since we are dealing with a real-time data stream. 

The rest of the prerequisites for the design and the implementation are the scalability, fault tolerance and extensibility of the system. All of the system's components should use distributed technologies to ensure scalability with the data stream throughput. Moreover, the technologies used should provide fault tolerance, since the system will be constantly running over extended periods of time, processing real-time data. Finally the system should be extensible, by allowing additional external datasets of any size to be joined with the data steam without the need for drastic changes in the implementation.

The use case for which we implement this system is the execution of SQL queries that join a real-time network data stream, generated by sampling IXP traffic, and external datasets containing Autonomous System and DNS information. The network data stream contains useful fields extracted from the headers of the sampled packet, whereas the external datasets can map IP addresses to Autonomous Systems and domain names. The critical difference between these two external datasets is their size, which as we will see affects the way the join can be performed. The specific queries that we intend to perform in this use case are topN AS and topN DNS queries, which return the top 10 Autonomous System and domain name pairs respectively for the IXP traffic over a specified time window.

The novelty of this thesis consists in combining state-of-the-art distributed technologies and techniques to minimize the execution latency of SQL queries that join a real-time data stream and an external dataset. We present an implementation that performs the join once during processing and allows subsequent queries to execute without the need to perform it again. Moreover, we provide a way of extending the system by adding external datasets of any size that are joined with the data stream. Finally, we apply a combination of optimizations that increase the system's performance.


\section{Related Work}

Over the recent years, various systems have been proposed to perform network analytics. In the following list we present some of them that are related to this thesis:
\begin{itemize}
\item \textbf{Datix} \cite{datix} is a distributed analytics system for network traffic data that relies on smart partitioning storage schemes to support fast join algorithms and efficient execution of filtering queries. However, Datix is built upon batch processing technologies such as MapReduce and Hive, which limits its scope to offline data processing. 
\item \textbf{Bro} \cite{bro} is a network monitoring framework that can be used for collecting and analyzing real-time network traffic. A Bro cluster can be deployed to achieve scalability. Unfortunately Bro does not integrate a storage solution for the processed data.
\item \textbf{DBStream} \cite{dbstream} is a real-time network traffic monitoring system which allows fast and flexible analysis across multiple data sources. It is based on the Data Stream Warehousing paradigm, which provides the means to handle both real-time and historical data. The crucial drawback of DBStream is that it is lacking scalability.
\item \textbf{CellIQ} \cite{celliq} is a real-time cellular network analytics system that supports complex analysis tasks. This system is not fit for our IXP traffic use case, because it is optimized for cellular network analytics, by leveraging the spatial and temporal locality cellular network data.
\item \textbf{FCCE} \cite{fcce} is a distributed, low latency key-value data management system. It is optimized to extract, store, retrieve, and correlate features from diverse data sources, including real-time data streams. While it can be used for our use case, FCCE does offer SQL support and only provides put and get operations similar to those of HBase.
\end{itemize}


\section{Thesis Outline}

In Chapter \ref{chapter:theory} we provide the necessary theoretical background so that the reader can familiarize themselves with the frameworks and technologies used in in the thesis. More specifically, we present the characteristics, architecture and key concepts of Kafka, Storm, HDFS, HBase and Phoenix.

In Chapter \ref{chapter:system} we describe the system's design and implementation. We provide a high-level overview of the system and its characteristics, followed by detailed information for its components, including the data generation and input part, the Kafka topic, the Storm topology and the Phoenix table.

In Chapter \ref{chapter:optimizations} we present the optimizations that we apply on the HBase cluster and the Phoenix table to increase the system's performance. More specifically, we describe the effects of HDFS short-circuit local reads, compression and data block encoding, disabling BlockCache on the Reverse DNS table and salting.

In Chapter \ref{chapter:evaluation} we evaluate the performance of the system. Firstly we describe the datasets used, as well as the evaluation cluster. Next, we perform experiments to evaluate the performance and the scalability of the Kafka, Storm, HBase and Phoenix components of the system.

In Chapter \ref{chapter:conclusion} we provide some concluding remarks as well as propositions for future work on the system.


\cleardoublepage
